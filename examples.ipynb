{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yargy\n",
    "import natasha\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pymorphy2 as pm\n",
    "\n",
    "from copy import deepcopy\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from collections import Counter\n",
    "from yargy import Parser\n",
    "\n",
    "from yargy_parser import MONEY\n",
    "from Model import Model\n",
    "from Preprocessor import Preprocessor\n",
    "from ForkExtractor import ForkExtractor\n",
    "from VacancyExtractor import VacancyExtractor\n",
    "\n",
    "import pickle\n",
    "\n",
    "preprocessor = Preprocessor()\n",
    "preprocessor.load()\n",
    "\n",
    "model = Model()\n",
    "model.load('./models')\n",
    "\n",
    "vac_extractor = VacancyExtractor(preprocessor=preprocessor)\n",
    "fork_extractor = ForkExtractor(preprocessor=preprocessor, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of jobs data (days): 1273 (years: 3.488)\n",
      "#progress: 0/1273\n",
      "#progress: 200/1273\n",
      "#progress: 400/1273\n",
      "#progress: 600/1273\n",
      "#progress: 800/1273\n",
      "#progress: 1000/1273\n",
      "#progress: 1200/1273\n",
      "Shape before droping short messages: (1093, 6)\n",
      "Shape after droping short messages: (1038, 8)\n",
      "Shape before droping rows with no any digit: (1038, 9)\n",
      "Shape after droping rows with no any digit: (1008, 9)\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "from data_loader import Dataloader\n",
    "\n",
    "def digit_extractor(txt):\n",
    "    tmp = re.search(r'(\\d)', txt, flags=re.IGNORECASE|re.DOTALL)\n",
    "    if tmp:\n",
    "        return True\n",
    "    return np.NaN\n",
    "\n",
    "DATA_FOLDER = 'data/'\n",
    "list_dir = os.listdir(DATA_FOLDER)\n",
    "data_len = len(list_dir)\n",
    "\n",
    "print(f'number of jobs data (days): {data_len} (years: {round(data_len/365, 3)})')\n",
    "\n",
    "seed = 253\n",
    "dataloader = Dataloader(DATA_FOLDER, random_state=seed)\n",
    "\n",
    "preprocessed_data = []\n",
    "\n",
    "for index, filename in enumerate(list_dir):\n",
    "    if index%200==0:\n",
    "        print (f'#progress: {index}/{data_len}')\n",
    "    posts = dataloader.parse_one_day(filename)\n",
    "    \n",
    "    reply_count = 0\n",
    "    for post in posts:\n",
    "        # filter out comments\n",
    "        if 'subtype' in post or not post.get('text', None):\n",
    "            continue\n",
    "        if post.get('reply_count', None) is None:\n",
    "            continue\n",
    "\n",
    "        preprocessed_data.append([filename] + [int(round(float(post[key]))) if key in ['ts', 'reply_count'] and post.get(key, None) is not None else post[key] if post.get(key, None) else np.NaN for key in ['text', 'user', 'ts', 'reply_count', 'reactions']])\n",
    "        \n",
    "df = pd.DataFrame(preprocessed_data, columns=['filename', 'text', 'user', 'ts', 'reply_count', 'reactions'])\n",
    "print(f'Shape before droping short messages: {df.shape}')\n",
    "# from timestamp to datetime\n",
    "df['ts_date'] = pd.to_datetime(df['ts'], unit='s')\n",
    "# measure the text lenght\n",
    "df['text_len'] = df['text'].str.len()\n",
    "# drop short messages\n",
    "df.drop(df[df['text_len']<200].index, inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "print(f'Shape after droping short messages: {df.shape}')\n",
    "\n",
    "df['is_any_digit'] = df['text'].apply(digit_extractor)\n",
    "print(f'Shape before droping rows with no any digit: {df.shape}')\n",
    "df.drop(df[df['is_any_digit'].isna()].index, inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "print(f'Shape after droping rows with no any digit: {df.shape}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train fork model if you'd like to train again models with your own feature set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#PROGRESS: 0/1008\n",
      "#PROGRESS: 50/1008\n",
      "#PROGRESS: 100/1008\n",
      "#PROGRESS: 150/1008\n",
      "#PROGRESS: 200/1008\n",
      "#PROGRESS: 250/1008\n",
      "#PROGRESS: 300/1008\n",
      "#PROGRESS: 350/1008\n",
      "#PROGRESS: 400/1008\n",
      "#PROGRESS: 450/1008\n",
      "#PROGRESS: 500/1008\n",
      "#PROGRESS: 550/1008\n",
      "#PROGRESS: 600/1008\n",
      "#PROGRESS: 650/1008\n",
      "#PROGRESS: 700/1008\n",
      "#PROGRESS: 750/1008\n",
      "#PROGRESS: 800/1008\n",
      "#PROGRESS: 850/1008\n",
      "#PROGRESS: 900/1008\n",
      "#PROGRESS: 950/1008\n",
      "#PROGRESS: 1000/1008\n",
      "null_indexes_len: 27, one_indexes_len: 208, multi_indexes_len: 773\n",
      "positive examples number: 179, negative examples number: 29\n",
      "implicit target assign error\n"
     ]
    }
   ],
   "source": [
    "messages = df['text'].values\n",
    "\n",
    "results = []\n",
    "indexes = []\n",
    "\n",
    "# extract from every message potential forks\n",
    "for index, message in enumerate(messages):\n",
    "    if index%50==0:\n",
    "        print(f'#PROGRESS: {index}/{len(messages)}')\n",
    "        \n",
    "    mes_forks = fork_extractor.parse(message)\n",
    "#     mes_forks structure e.g.:\n",
    "#         100 - 250k$ per month, ... [[100, 250, 'USD', 1000, 'month', span.start, span.end], ...]\n",
    "#         Salary: ~100 ... [[100, -1, '-', -1, '-', span.start, span.end], ...]\n",
    "\n",
    "    indexes.append(index)\n",
    "    results.append(mes_forks)\n",
    "        \n",
    "# determine indexes of messages where:\n",
    "# null_indexes - no found any forks based on created yargy.Parser(MONEY)\n",
    "# one_indexes - found only 1 fork per message\n",
    "# multi_indexes - found multiple forks per message\n",
    "null_indexes, one_indexes, multi_indexes = fork_extractor.group_parsed_messages(results)\n",
    "print(f'null_indexes_len: {len(null_indexes)}, one_indexes_len: {len(one_indexes)}, multi_indexes_len: {len(multi_indexes)}')\n",
    "\n",
    "preprocessor = Preprocessor()\n",
    "# label `one_indexes` messages forks based on some specified rules defined in preprocessor.__unsupervised_parse_error_detect method\n",
    "# positive - indexes of messages where fork is legitimate. Message e.g. \"Вилка от 100 до бесконечности\"\n",
    "# negative - indexes of messages where fork is not legitimate. Message e.g. \"В нашей команде 5 человек\"\n",
    "positive, negative = preprocessor.one_index_unsupervised_parse_error_detect(one_indexes, results, messages)\n",
    "print(f'positive examples number: {len(positive)}, negative examples number: {len(negative)}')\n",
    "# as you can see - the dataset is a little bit imbalanced\n",
    "\n",
    "# create train dataset with some feature engineering\n",
    "lcl_df = preprocessor.prepare_train_dataset(positive, negative, results, messages)\n",
    "\n",
    "\n",
    "with open('./preprocessor.pkl', 'wb') as file:\n",
    "    pickle.dump([preprocessor.pos_words, # words in positive messages\n",
    "                 preprocessor.neg_words, # words in negative messages\n",
    "                 preprocessor.ohe_dc, \n",
    "                 preprocessor.ohe_out_columns], file)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define your own model with your own feature set\n",
    "\n",
    "I trained Decision tree ({'max_depth': 4}) and Logistic Regression ('solver': 'liblinear').\n",
    "\n",
    "All features that have been engineered during `prepare_train_dataset` step:\n",
    "    `2gte1`, - is the second value of fork >= the first one\n",
    "    `len1`, - length of the first value of fork\n",
    "    `1mod5==0`, - is the first value of fork `mod` 5 == 0\n",
    "    `1<10`, - is the first value of fork < 10\n",
    "    `2==-1`, - is the second value of fork the default value (-1)\n",
    "    `len2`, - length of the second value of fork\n",
    "    `2mod5==0`, - is the second value of fork `mod` 5 == 0\n",
    "    `2<10`, - is the second value of fork < 10\n",
    "    `curMultPeriod`, - is yargy.Parser detect any either `currency` or `multiplier` or `period`\n",
    "    `only_digit`, - is the anything except of degitits in predefined `local_context`\n",
    "    `contex`, - yargy.Parser() span +- 50 chars in both sides\n",
    "    `local_context`, - ....\n",
    "    `len_tokens`, - number of `tokens` in `context`\n",
    "    `tokens`,  - ...\n",
    "    `all_words_score`, - just look at the source code to understand how i calculated `all_words` and `short_words` scores. Intuition behind it - it reflects how the context correspond to the extracted yargy.Parser structure. In short, its calculation depends on `context` `tokens`, its count appearance in `positive` and `negative` examples and it's all weighted based on the distance to the yargy.Parser extracted strcture. \n",
    "    `short_words_score`\n",
    "    \n",
    "    \n",
    "The chosen features for both models are in the source code of Model() class.\n",
    "Validation strategy - leave one out, because the training set is small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#MODEL: training start\n",
      "Base models training\n",
      "TRAIN: Acc: 0.9951690821256035, Roc_auc: 0.9990028592928535, PRC_auc: 0.9998331874576499\n",
      "TEST: Acc: 0.9663461538461539\n",
      "TRAIN: Acc: 0.9764725009290228, Roc_auc: 0.9680445051433092, PRC_auc: 0.988056589275073\n",
      "TEST: Acc: 0.9711538461538461\n",
      "Stacked models training\n",
      "TRAIN: Acc: 1.0, Roc_auc: 1.0, PRC_auc: 1.0\n",
      "TEST: Acc: 0.9951923076923077\n",
      "TRAIN: Acc: 0.9902684875510962, Roc_auc: 0.9984983029631341, PRC_auc: 0.9997441100377977\n",
      "TEST: Acc: 0.9855769230769231\n"
     ]
    }
   ],
   "source": [
    "# define your own model with your own feature set\n",
    "model = Model()\n",
    "model.train(lcl_df)\n",
    "\n",
    "#save your model\n",
    "# model.save('./models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "lcl_df = model.predict(lcl_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If you'd like to look at the error of models then see below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TREE MODEL ERROR\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>context</th>\n",
       "      <th>tokens</th>\n",
       "      <th>all_words_score</th>\n",
       "      <th>short_words_score</th>\n",
       "      <th>target</th>\n",
       "      <th>pred_proba_linear</th>\n",
       "      <th>pred_label_linear</th>\n",
       "      <th>pred_proba_tree</th>\n",
       "      <th>pred_label_tree</th>\n",
       "      <th>pred_proba_linear_stack</th>\n",
       "      <th>pred_proba_tree_stack</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>100</td>\n",
       "      <td>-1</td>\n",
       "      <td>-</td>\n",
       "      <td>-1</td>\n",
       "      <td>-</td>\n",
       "      <td>Официальное трудоустройство и 100% официальная...</td>\n",
       "      <td>['официальный', 'трудоустройство', 'и', '100%'...</td>\n",
       "      <td>0.596345</td>\n",
       "      <td>0.749624</td>\n",
       "      <td>0</td>\n",
       "      <td>0.69456</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.835748</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      1  2  3  4  5                                            context  \\\n",
       "25  100 -1  - -1  -  Официальное трудоустройство и 100% официальная...   \n",
       "\n",
       "                                               tokens  all_words_score  \\\n",
       "25  ['официальный', 'трудоустройство', 'и', '100%'...         0.596345   \n",
       "\n",
       "    short_words_score  target  pred_proba_linear  pred_label_linear  \\\n",
       "25           0.749624       0            0.69456                1.0   \n",
       "\n",
       "    pred_proba_tree  pred_label_tree  pred_proba_linear_stack  \\\n",
       "25         0.909091              1.0                 0.835748   \n",
       "\n",
       "    pred_proba_tree_stack  \n",
       "25                    0.0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('TREE MODEL ERROR')\n",
    "lcl_df[lcl_df['pred_label_tree']!=lcl_df['target']][['1', '2', '3', '4', '5', 'context', 'tokens', 'all_words_score',\n",
    "       'short_words_score', 'target', 'pred_proba_linear', 'pred_label_linear', 'pred_proba_tree', 'pred_label_tree', 'pred_proba_linear_stack', 'pred_proba_tree_stack']].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LINEAR MODEL ERROR\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>context</th>\n",
       "      <th>tokens</th>\n",
       "      <th>all_words_score</th>\n",
       "      <th>short_words_score</th>\n",
       "      <th>target</th>\n",
       "      <th>pred_proba_linear</th>\n",
       "      <th>pred_label_linear</th>\n",
       "      <th>pred_proba_tree</th>\n",
       "      <th>pred_label_tree</th>\n",
       "      <th>pred_proba_linear_stack</th>\n",
       "      <th>pred_proba_tree_stack</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>20</td>\n",
       "      <td>-1</td>\n",
       "      <td>-</td>\n",
       "      <td>-1</td>\n",
       "      <td>-</td>\n",
       "      <td>плюшки стандартны, правило 20%  в нашем случае...</td>\n",
       "      <td>['плюшка', 'стандартный', 'правило', '20%', 'в...</td>\n",
       "      <td>0.337780</td>\n",
       "      <td>0.545446</td>\n",
       "      <td>0</td>\n",
       "      <td>0.592706</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.215607</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>100</td>\n",
       "      <td>-1</td>\n",
       "      <td>-</td>\n",
       "      <td>-1</td>\n",
       "      <td>-</td>\n",
       "      <td>Официальное трудоустройство и 100% официальная...</td>\n",
       "      <td>['официальный', 'трудоустройство', 'и', '100%'...</td>\n",
       "      <td>0.596345</td>\n",
       "      <td>0.749624</td>\n",
       "      <td>0</td>\n",
       "      <td>0.694560</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.835748</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>-</td>\n",
       "      <td>-1</td>\n",
       "      <td>-</td>\n",
       "      <td>С точки зрения задач, примерно 50/50 это анали...</td>\n",
       "      <td>['с', 'точка', 'зрение', 'задача', 'примерно',...</td>\n",
       "      <td>0.247281</td>\n",
       "      <td>0.247281</td>\n",
       "      <td>0</td>\n",
       "      <td>0.779783</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.492163</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>15000</td>\n",
       "      <td>4000</td>\n",
       "      <td>USD</td>\n",
       "      <td>-1</td>\n",
       "      <td>-</td>\n",
       "      <td>вилка зарплат(от 15000 $ до 4000$)</td>\n",
       "      <td>['вилка', 'зарплата', 'от', '15000', 'до', '40...</td>\n",
       "      <td>1.852960</td>\n",
       "      <td>2.667786</td>\n",
       "      <td>0</td>\n",
       "      <td>0.998520</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.897424</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>20</td>\n",
       "      <td>-1</td>\n",
       "      <td>-</td>\n",
       "      <td>-1</td>\n",
       "      <td>-</td>\n",
       "      <td>Есть предложение тех, кто собрал 20 :ban: под ...</td>\n",
       "      <td>['есть', 'предложение', 'тот', 'кто', 'собрать...</td>\n",
       "      <td>0.157154</td>\n",
       "      <td>0.273504</td>\n",
       "      <td>0</td>\n",
       "      <td>0.501762</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.192746</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         1     2    3  4  5  \\\n",
       "18      20    -1    - -1  -   \n",
       "25     100    -1    - -1  -   \n",
       "27      50    50    - -1  -   \n",
       "28   15000  4000  USD -1  -   \n",
       "207     20    -1    - -1  -   \n",
       "\n",
       "                                               context  \\\n",
       "18   плюшки стандартны, правило 20%  в нашем случае...   \n",
       "25   Официальное трудоустройство и 100% официальная...   \n",
       "27   С точки зрения задач, примерно 50/50 это анали...   \n",
       "28                  вилка зарплат(от 15000 $ до 4000$)   \n",
       "207  Есть предложение тех, кто собрал 20 :ban: под ...   \n",
       "\n",
       "                                                tokens  all_words_score  \\\n",
       "18   ['плюшка', 'стандартный', 'правило', '20%', 'в...         0.337780   \n",
       "25   ['официальный', 'трудоустройство', 'и', '100%'...         0.596345   \n",
       "27   ['с', 'точка', 'зрение', 'задача', 'примерно',...         0.247281   \n",
       "28   ['вилка', 'зарплата', 'от', '15000', 'до', '40...         1.852960   \n",
       "207  ['есть', 'предложение', 'тот', 'кто', 'собрать...         0.157154   \n",
       "\n",
       "     short_words_score  target  pred_proba_linear  pred_label_linear  \\\n",
       "18            0.545446       0           0.592706                1.0   \n",
       "25            0.749624       0           0.694560                1.0   \n",
       "27            0.247281       0           0.779783                1.0   \n",
       "28            2.667786       0           0.998520                1.0   \n",
       "207           0.273504       0           0.501762                1.0   \n",
       "\n",
       "     pred_proba_tree  pred_label_tree  pred_proba_linear_stack  \\\n",
       "18          0.000000              0.0                 0.215607   \n",
       "25          0.909091              1.0                 0.835748   \n",
       "27          0.000000              0.0                 0.492163   \n",
       "28          0.000000              0.0                 0.897424   \n",
       "207         0.000000              0.0                 0.192746   \n",
       "\n",
       "     pred_proba_tree_stack  \n",
       "18                     0.0  \n",
       "25                     0.0  \n",
       "27                     0.0  \n",
       "28                     0.0  \n",
       "207                    0.0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('LINEAR MODEL ERROR')\n",
    "lcl_df[lcl_df['pred_label_linear']!=lcl_df['target']][['1', '2', '3', '4', '5', 'context', 'tokens', 'all_words_score',\n",
    "       'short_words_score', 'target', 'pred_proba_linear', 'pred_label_linear', 'pred_proba_tree', 'pred_label_tree', 'pred_proba_linear_stack', 'pred_proba_tree_stack']].head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## As you can see the models most of the time make the false predictions on forks with the context containing '%' symbol. That's why i made some post filtering based on some rules in the source code of ForkExtractor. Look in the extract method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples of Fork and Vacancy parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('от 60К руб net до 300К net', '60000 RUB-300000 RUB') [[60000.0, -1.0, 'RUB'], [300000.0, -1.0, '-']]\n",
      "('от 60К до 300К грязными', '60000 RUB-300000 RUB') [[60000.0, 300000.0, '-']]\n",
      "('от 60к до 300к gross', '60000 RUB-300000 RUB') [[60000.0, 300000.0, '-']]\n",
      "('120т.р. - 160 т.р. чистыми', '120000 RUB-160000 RUB') [[120000.0, 160000.0, 'RUB']]\n",
      "('$5k–$8k', '5000 USD-8000 USD') [[5000.0, 8000.0, 'USD']]\n",
      "('150-250 т.р. «чистыми»', '150000 RUB-250000 RUB') [[150000.0, 250000.0, 'RUB']]\n",
      "('2.5-4.5k USD', '2500.0 USD-4500.0 USD') [[4000.0, 5000.0, 'USD']]\n",
      "('2.5-4.5k $', '2500.0 USD-4500.0 USD') [[4000.0, 5000.0, 'USD']]\n",
      "('2.5-4.5k$', '2500.0 USD-4500.0 USD') [[4000.0, 5000.0, 'USD']]\n",
      "('1K - 2K EUR нетто ', '1000 EUR-2000 EUR') [[1000.0, 2000.0, 'EUR']]\n",
      "('1K - 2K € нетто ', '1000 EUR-2000 EUR') [[1000.0, 2000.0, 'EUR']]\n",
      "('1K - 2K€ нетто ', '1000 EUR-2000 EUR') [[1000.0, 2000.0, 'EUR']]\n",
      "('€1K - €2K EUR нетто ', '1000 EUR-2000 EUR') [[1000.0, 2000.0, 'EUR']]\n",
      "('1K - 2K € нетто ', '1000 EUR-2000 EUR') [[1000.0, 2000.0, 'EUR']]\n",
      "('1000 - 2000 € нетто ', '1000 EUR-2000 EUR') [[1000.0, 2000.0, 'EUR']]\n",
      "('Оклад в вилке от 150 до 250 гросс', '150000 RUB-250000 RUB') [[150000.0, 250000.0, '-']]\n",
      "('ЗП: 130-200к руб.', '130000 RUB-200000 RUB') [[130000.0, 200000.0, 'RUB']]\n",
      "('Зарплату от 200К до 1М рублей', '200000 RUB-1000000 RUB') []\n",
      "('зп: 60 000 - 120 000 т.р. net', '60000 RUB-120000 RUB') []\n",
      "('от 3,4 до 4,8 млн.рублей', '3400000 RUB-4800000 RUB') []\n",
      "('280-400+ тысяч рублей', '280000 RUB-400000 RUB') [[280000.0, 400000.0, 'RUB']]\n",
      "('вилка $$1000-5000', '1000 USD-5000 USD') [[1000.0, 5000.0, 'USD']]\n",
      "('1000-2500k USD', '1000 USD-2500 USD') []\n",
      "('от $ 800 до 1100 net', '800 USD-1100 USD') [[800.0, 1100.0, 'USD']]\n",
      "('от 3,4 до 4,8 млн.рублей', '3400000 RUB-4800000 RUB') []\n",
      "('280+ тысяч рублей', '280000 RUB-400000 RUB') [[280000.0, -1.0, 'RUB']]\n",
      "('вилка $1000', '1000 USD-5000 USD') [[1000.0, -1.0, 'USD']]\n",
      "('вилка рублей 21 000 + бонусы', '1000 USD-5000 USD') [[21000.0, -1.0, 'RUB']]\n",
      "('2500k USD', '1000 USD-2500 USD') []\n",
      "('50-259k рублей (jun-senior 50-120, 120-200, 200 -250)', '1000 USD-2500 USD') [[50000.0, 259000.0, 'RUB'], [50000.0, 120000.0, '-'], [120000.0, 200000.0, '-'], [200000.0, 250000.0, '-']]\n",
      "------\n",
      "+7(495)6386767 []\n",
      "tel:+7(906)747-73-90 []\n",
      "2018/01/29 []\n",
      "Более 20 000 сотрудников по всей России [[20000.0, -1.0, '-']]\n",
      "http://andrewgelman.com/2017/01/16/hiring-hiring-hiring-hiring/ []\n",
      "Белая зп.: 150 000 рублей [[150000.0, -1.0, 'RUB']]\n",
      "График работы с 9:30 до 18:00 []\n",
      "мы планируем вырасти с 1,5 до 50 миллионов пользователей []\n",
      "Equity range: 0.25-1.5% []\n"
     ]
    }
   ],
   "source": [
    "\n",
    "fork_extractor = ForkExtractor(preprocessor=preprocessor, model=model)\n",
    "\n",
    "# fork_extractor extract method return 2 list:\n",
    "# 1 list - yargy.parser captured list of forks. During yargy parsing i didn't consider gross/net property.\n",
    "# 2 list - filtered out list of normalized forks of the 1 list. E.g. it transform sallary per hour/day/year to month format and multiply according to multiplier\n",
    "\n",
    "cases_fork = [('от 60К руб net до 300К net', '60000 RUB-300000 RUB'),\n",
    "# in the case above due to the yargy.Parser didn't consider net/gross property, it splits one fork into 2 seperate forks\n",
    "                ('от 60К до 300К грязными', '60000 RUB-300000 RUB'),\n",
    "              ('от 60к до 300к gross', '60000 RUB-300000 RUB'),\n",
    "              ('120т.р. - 160 т.р. чистыми', '120000 RUB-160000 RUB'),\n",
    "              ('$5k–$8k', '5000 USD-8000 USD'),\n",
    "              ('150-250 т.р. «чистыми»', '150000 RUB-250000 RUB'),\n",
    "              ('2.5-4.5k USD', '2500.0 USD-4500.0 USD'),\n",
    "              ('2.5-4.5k $', '2500.0 USD-4500.0 USD'),\n",
    "              ('2.5-4.5k$', '2500.0 USD-4500.0 USD'),\n",
    "              ('1K - 2K EUR нетто ', '1000 EUR-2000 EUR'),\n",
    "              ('1K - 2K € нетто ', '1000 EUR-2000 EUR'),\n",
    "              ('1K - 2K€ нетто ', '1000 EUR-2000 EUR'),\n",
    "              ('€1K - €2K EUR нетто ', '1000 EUR-2000 EUR'),\n",
    "              ('1K - 2K € нетто ', '1000 EUR-2000 EUR'),\n",
    "              ('1000 - 2000 € нетто ', '1000 EUR-2000 EUR'),\n",
    "              ('Оклад в вилке от 150 до 250 гросс', '150000 RUB-250000 RUB'),\n",
    "              ('ЗП: 130-200к руб.', '130000 RUB-200000 RUB'),\n",
    "              ('Зарплату от 200К до 1М рублей', '200000 RUB-1000000 RUB'),\n",
    "              # ForkExtracotr filters out values bigger than 700k rubles and 200k$ if there is no mention about the sallary `per year`\n",
    "              # in order to avoid false capturing like `we have 20 milions of users` or `we got 980k$ round investments`\n",
    "              ('зп: 60 000 - 120 000 т.р. net', '60000 RUB-120000 RUB'),\n",
    "              ('от 3,4 до 4,8 млн.рублей', '3400000 RUB-4800000 RUB'),\n",
    "              ('280-400+ тысяч рублей', '280000 RUB-400000 RUB'),\n",
    "              ('вилка $$1000-5000', '1000 USD-5000 USD'),\n",
    "              ('1000-2500k USD', '1000 USD-2500 USD'), \n",
    "              # ForkExtractor interprets the values like above: [ 1000, 2500, USD, 1000] -> [ 1000 000, 2500 000, USD]\n",
    "              ('от $ 800 до 1100 net', '800 USD-1100 USD'),\n",
    "              ('от 3,4 до 4,8 млн.рублей', '3400000 RUB-4800000 RUB'),\n",
    "              ('280+ тысяч рублей', '280000 RUB-400000 RUB'),\n",
    "              ('вилка $1000', '1000 USD-5000 USD'),\n",
    "              ('вилка рублей 21 000 + бонусы', '1000 USD-5000 USD'),\n",
    "              ('2500k USD', '1000 USD-2500 USD'),\n",
    "             ('50-259k рублей (jun-senior 50-120, 120-200, 200 -250)', '1000 USD-2500 USD'),]\n",
    "\n",
    "cases_not_fork = ['+7(495)6386767',\n",
    "                  'tel:+7(906)747-73-90',\n",
    "                  '2018/01/29',\n",
    "                  'Более 20 000 сотрудников по всей России',\n",
    "                  'http://andrewgelman.com/2017/01/16/hiring-hiring-hiring-hiring/',\n",
    "                  'Белая зп.: 150 000 рублей',\n",
    "                  'График работы с 9:30 до 18:00',\n",
    "                  'мы планируем вырасти с 1,5 до 50 миллионов пользователей',\n",
    "                  'Equity range: 0.25-1.5%']\n",
    "\n",
    "for x in cases_fork:\n",
    "    print(x, fork_extractor.extract(x[0])[1])\n",
    "print('------')\n",
    "for x in cases_not_fork:\n",
    "    print(x, fork_extractor.extract(x)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Junior ML Engineer ([['junior', ('ml',), 'engineer', 0, 18, '']], [['junior', ('ml',), 'engineer']]) \n",
      "-----\n",
      "Jun/senior ds ([['junior', (), 'ds', 0, 13, ''], ['senior', (), 'ds', 0, 13, '']], [['junior', (), 'ds'], ['senior', (), 'ds']]) \n",
      "-----\n",
      "Jun/mid/senior (nlp/cv) ds ([['junior', ('nlp', 'cv'), 'ds', 0, 26, ''], ['middle', ('nlp', 'cv'), 'ds', 0, 26, ''], ['senior', ('nlp', 'cv'), 'ds', 0, 26, '']], [['junior', ('nlp', 'cv'), 'ds'], ['middle', ('nlp', 'cv'), 'ds'], ['senior', ('nlp', 'cv'), 'ds']]) \n",
      "-----\n",
      "ds Jun/mid/senior (nlp/cv) ([['junior', ('nlp',), 'ds', 0, 22, ''], ['middle', ('nlp',), 'ds', 0, 22, ''], ['senior', ('nlp',), 'ds', 0, 22, '']], [['junior', ('nlp',), 'ds'], ['middle', ('nlp',), 'ds'], ['senior', ('nlp',), 'ds']]) \n",
      "-----\n",
      "ds (nlp/cv) Jun/mid/senior ([['', ('nlp',), 'ds', 0, 7, '']], [['', ('nlp',), 'ds']]) \n",
      "-----\n",
      "Jun/mid/senior ds (nlp/cv) ([['junior', ('nlp',), 'ds', 0, 22, ''], ['middle', ('nlp',), 'ds', 0, 22, ''], ['senior', ('nlp',), 'ds', 0, 22, '']], [['junior', ('nlp',), 'ds'], ['middle', ('nlp',), 'ds'], ['senior', ('nlp',), 'ds']]) \n",
      "-----\n",
      "Ваша задача будет менторить Jun ds-ов и делать код ревью ([['junior', (), 'ds', 28, 35, 'Ваша задача будет менторить ']], []) \n",
      "-----\n",
      "Мы ищем кучу Jun ds-ов, чтобы они батрачали на нас ([['junior', (), 'ds', 13, 20, 'Мы ищем кучу ']], [['junior', (), 'ds']]) \n",
      "-----\n",
      "Ведется набор Jun ds-ов, чтобы они батрачали на нас ([['junior', (), 'ds', 14, 21, 'Ведется набор ']], []) \n",
      "-----\n",
      "Вы владеете стандартным набором ds утилиток ([['', (), 'ds', 32, 35, 'Вы владеете стандартным набором ']], []) \n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "vac_extractor = VacancyExtractor(preprocessor=preprocessor)\n",
    "# VacancyExtractor extractor based on regexps\n",
    "# VacancyExtractor regexp patern - level_re + field_re + vac_name_re + level_re + field_re\n",
    "\n",
    "cases = ['Junior ML Engineer',\n",
    "        'Jun/senior ds',\n",
    "        'Jun/mid/senior (nlp/cv) ds',\n",
    "        'ds Jun/mid/senior (nlp/cv)', # in case when fields are in the end and there are multiple fields - the last field isn't caught\n",
    "        'ds (nlp/cv) Jun/mid/senior', # doesn't work, because it didn't correspond the regexp pattern. But usually nobody writes in such format\n",
    "        'Jun/mid/senior ds (nlp/cv)', \n",
    "        'Ваша задача будет менторить Jun ds-ов и делать код ревью', \n",
    "        'Мы ищем кучу Jun ds-ов, чтобы они батрачали на нас', \n",
    "        'Ведется набор Jun ds-ов, чтобы они батрачали на нас', \n",
    "        'Вы владеете стандартным набором ds утилиток', \n",
    "        ]\n",
    "\n",
    "# VacancyExtractor.extract returns 2 lists\n",
    "\n",
    "# the first list is unfiltered set of captured vacancies sorted by VacancyExtractor.__vacany_sort_func1\n",
    "# The first list structure : [[level, (field, field, ...), vacancy_name, span.start, span.end, left_context]]\n",
    "\n",
    "# the second list is filtered set of captured UNIQUE vacancies based on the first list. It filters based on the left context:\n",
    "# if no left context of there is not any alphabet character in the left context\n",
    "# either in the left context there is such lemmatized parts of words:\n",
    "# ['vaca', 'posi', 'role', 'need', 'look', 'job', 'вака', 'назв', 'роль', 'треб', 'нужн', 'поис', 'разы', 'иска', 'необ'] or 'позици' in tkn and 'позицио' not in tkn:\n",
    "# Due to the last 2 cases, i didn't inclue 'набо' in the filter list\n",
    "\n",
    "for x in cases:\n",
    "    print(x, vac_extractor.extract(x), '\\n-----')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract all vacancy names and forks for our DF vacancies (Optional. If you want to continue the project and try to link multple vacancies to corresponding multiple forks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "messages = df['text'].values\n",
    "vacancies = [[], []]\n",
    "indexes = []\n",
    "\n",
    "for index, message in enumerate(messages):\n",
    "    if index%50==0:\n",
    "        print(f'#PROGRESS: {index}/{len(messages)}')\n",
    "    try:\n",
    "        mes_forks, mes_normalized_forks = fork_extractor.extract(message)\n",
    "        mes_vacancies, mes_filt_vacancies = vac_extractor.extract(message)\n",
    "\n",
    "        if [] in [mes_forks, mes_normalized_forks, mes_vacancies, mes_filt_vacancies]:\n",
    "            continue\n",
    "\n",
    "        indexes.append(index)\n",
    "        forks[0].append(mes_forks)\n",
    "        forks[1].append(mes_normalized_forks)\n",
    "        vacancies[0].append(mes_vacancies)\n",
    "        vacancies[1].append(mes_filt_vacancies)\n",
    "    except Exception as e:\n",
    "        print(index, e)\n",
    "        assert False\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
